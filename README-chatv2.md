

## ğŸ“˜ `README-chatv2.md` â€” Interface avancÃ©e `chatv2-kortex.sh`


# ğŸ§  chatv2-kortex.sh

`chatv2-kortex.sh` est un script Bash interactif conÃ§u pour lancer une session de dialogue fluide avec SMKortex, un assistant IA local alimentÃ© par `llama.cpp`.

Câ€™est la version la plus optimisÃ©e des interfaces disponibles, intÃ©grant :

- ğŸ“ un contrÃ´le fin sur les paramÃ¨tres de gÃ©nÃ©ration
- ğŸ›ï¸ une configuration facile en dÃ©but de fichier
- ğŸ¨ des couleurs lisibles dans le terminal
- ğŸªµ un journal automatique de chaque session (`logs/`)

---

## â–¶ï¸ Lancer une session

Depuis la racine du projet :

```bash
./scripts/chatv2-kortex.sh
```

---

## ğŸ”§ ParamÃ¨tres configurables

En haut du fichier, les variables suivantes peuvent Ãªtre modifiÃ©es facilement :

| Variable        | Description                                 | Exemple                         |
|----------------|---------------------------------------------|---------------------------------|
| `MODEL`         | Chemin du modÃ¨le `.gguf`                   | `./llama/models/vigogne.gguf`   |
| `BIN`           | Chemin du binaire `llama-cli`              | `./llama/llama.cpp/build/bin/llama-cli` |
| `N_PREDICT`     | Nombre max de tokens gÃ©nÃ©rÃ©s               | `200`                           |
| `TEMPERATURE`   | TempÃ©rature de gÃ©nÃ©ration (`--temp`)       | `0.5`                           |
| `TOP_P`         | Top-p sampling                             | `0.9`                           |
| `REVERSE_PROMPT`| Prompt dÃ©tectant la fin de rÃ©ponse         | `"Utilisateur :"`              |

---

## ğŸ¯ Comportement

- Charge un prompt systÃ¨me au lancement pour â€œorienterâ€ le modÃ¨le
- Affiche une invite lisible : `Utilisateur :`
- GÃ©nÃ¨re une rÃ©ponse propre depuis le modÃ¨le local
- Affiche la rÃ©ponse **dans le terminal** ET la **log dans `logs/`**
- Utilise `tee -a` pour un affichage temps rÃ©el

---

## ğŸ—‚ï¸ Exemple de structure de projet

```
.
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ chatv2-kortex.sh
â”œâ”€â”€ logs/
â”‚   â””â”€â”€ session_14-03_01-07-2025.log
â”œâ”€â”€ llama/
â”‚   â”œâ”€â”€ llama.cpp/
â”‚   â””â”€â”€ models/
â”‚       â””â”€â”€ vigogne-2-7b-chat.Q4_K_M.gguf
â”œâ”€â”€ context.txt
â”œâ”€â”€ setup-smkortex.sh
â””â”€â”€ README.md
```

---

## ğŸ’¡ Astuces

- Tu peux crÃ©er un alias :
  ```bash
  alias smk='./scripts/chatv2-kortex.sh'
  ```

- Pour tester sans log :
  Supprime ou commente l'appel `tee -a "$LOGFILE"` et laisse seulement `| cat`

- Pour booster les performances :
  Utilise `make -j$(nproc)` Ã  la compilation et augmente `n_threads` dans `llama.cpp`

---

## ğŸ§ª Ã€ venir

- Ajout dâ€™une configuration externe (`smk.conf`)
- Commandes internes (`!reset`, `!log off`)
- Mode turbo ou crÃ©atif activable Ã  la volÃ©e
- Affichage TUI avec `dialog`, `gum`, ou `fzf`

---

ğŸ’¬ En cas de souci, nâ€™oublie pas de vÃ©rifier ton prompt, ton modÃ¨le et lâ€™option `--temp` selon ta version de `llama-cli`.  
Et surtout : amuse-toi Ã  le faire Ã©voluer ğŸ› ï¸ğŸ’š

