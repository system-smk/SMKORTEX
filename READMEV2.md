
# ğŸ§  SMKortex â€” Assistant local basÃ© sur LLaMA

SMKortex est une interface locale personnalisÃ©e autour dâ€™un modÃ¨le LLaMA (comme Vigogne 2), intÃ©grant des scripts Bash simples, des logs automatiques, et un sous-module `llama.cpp`.

---

## ğŸ“‚ Structure du projet

```
smkortex/
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ chat-smkortex.sh       # Lance l'assistant de faÃ§on basique
â”‚   â””â”€â”€ front-smkortex.sh      # Interface ligne par ligne, avec historique
â”œâ”€â”€ llama/
â”‚   â””â”€â”€ llama.cpp/             # Sous-module Git (llama.cpp officiel)
â”œâ”€â”€ models/
â”‚   â””â”€â”€ vigogne-2-7b-chat.Q4_K_M.gguf   # ModÃ¨le GGUF ici
â”œâ”€â”€ logs/
â”‚   â””â”€â”€ session_...log         # Historique de session
â”œâ”€â”€ context.txt                # MÃ©moire de session (front-smkortex)
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md
```

---

## ğŸ”§ Installation

### 1. Cloner ce dÃ©pÃ´t avec le sous-module

```bash
git clone https://github.com/ton-utilisateur/smkortex.git
cd smkortex
git submodule update --init --recursive
```

### 2. Compiler llama.cpp

```bash
cd llama/llama.cpp
make
cd ../../
```

L'exÃ©cutable se trouve ici :
```
llama/llama.cpp/build/bin/llama-cli
```

### 3. Placer un modÃ¨le `.gguf`

TÃ©lÃ©charge le modÃ¨le (ex. Vigogne 2) et place-le dans :

```
models/vigogne-2-7b-chat.Q4_K_M.gguf
```

---

## ğŸš€ Lancer SMKortex

### â–¸ Avec `chat-smkortex.sh`

```bash
./scripts/chat-smkortex.sh
```

- DÃ©marrage rapide
- Prompt systÃ¨me prÃ©dÃ©fini
- Log automatique dans `logs/`
- RÃ©ponses limitÃ©es Ã  256 tokens

### â–¸ Avec `front-smkortex.sh`

```bash
./scripts/front-smkortex.sh
```

- Invite claire `<|UTILISATEUR|>:`
- Log complet des Ã©changes
- Historique de session dans `context.txt` (mÃ©moire)
- Dialogue ligne Ã  ligne avec relance

---



## â¤ï¸ CrÃ©dits

- Vigogne 2 : https://github.com/bigscience-workshop/vigogne
- llama.cpp : https://github.com/ggerganov/llama.cpp
```

---
