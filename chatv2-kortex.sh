#!/bin/bash

# === Configuration utilisateur ===
MODEL="./llama/models/vigogne-2-7b-chat.Q4_K_M.gguf"
BIN="./llama/llama.cpp/build/bin/llama-cli"
N_PREDICT=200
TEMPERATURE=0.5
TOP_P=0.9
REVERSE_PROMPT="Utilisateur :"
LOGFILE="logs/session_$(date +'%H-%M_%d-%m-%Y').log"

# === Prompt systÃ¨me renforcÃ© ===
PROMPT_SYSTEM="Tu es un assistant IA francophone, factuel et bienveillant.\n\nUtilisateur : bonjour\nAssista>

# === Interface ===
clear
echo -e "\nðŸ§  SMKortex v2 â€” Session interactive"
echo "ðŸ“… Log : $LOGFILE"
echo "âœ  Ctrl+C pour quitter"

# === Boucle interactive ===
while true; do
  echo -ne "\n\033[1;36mUtilisateur :\033[0m "
  read -e INPUT

  echo "Utilisateur : $INPUT" >> "$LOGFILE"
  echo -e "\033[1;34m\nðŸ’¬ SMKortex rÃ©pond...\033[0m"

  PROMPT="$PROMPT_SYSTEM\nUtilisateur : $INPUT\nAssistant:"

  "$BIN" \
    -m "$MODEL" \
    --prompt "$PROMPT" \
    --n-predict "$N_PREDICT" \
    --temp "$TEMPERATURE" \
    --top-p "$TOP_P" \
    --reverse-prompt "$REVERSE_PROMPT" \
    --color | tee -a "$LOGFILE"
done
